{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Econ 8210 Quant Macro, Homework 1\n",
    "## Part 2 - Solution Methods\n",
    "Haosi Shen, Fall 2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Housekeeping\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import torch\n",
    "\n",
    "np.random.seed(42) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing Pareto Efficient Allocations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider an endowment economy with $m$ different goods and $n$ agents. Each agent $i = 1, ..., n$ has an endowment $e_j^i >0$ for every $j = 1, ..., m$ and a utility function of the form\n",
    "$$ u^i (x) = \\sum_{j=1}^{m} \\alpha_j \\frac{x_{j}^{1+\\omega_j^i}}{1+\\omega_j^i} $$\n",
    "where $\\alpha_j > 0 > \\omega_j^i$ are agent-specific parameters.\n",
    "\n",
    "Given some social weights $\\lambda_i > 0$, solve for the social plannerâ€™s problem for $m = n = 3$ using the **Adam (Adaptive Moment Estimation)** method. Try different values of $\\alpha_j,\\;\\omega_j^i,\\;\\lambda_i$. \n",
    "\n",
    "$$ \\max_{\\{x^i\\}} \\; \\sum_{i=1}^{n} \\lambda_i u^{i}(x)$$\n",
    "\n",
    "Compute first the case where all the agents have the same parameters and\n",
    "social weights and later a case where there is a fair degree of heterogeneity.\n",
    "\n",
    "How does the method perform? How does heterogeneity in the agent-specific parameters\n",
    "affect the results?\n",
    "\n",
    "Can you handle the case where $m = n = 10$?\n",
    "\n",
    "> I choose to use **Adam** since it is more efficient for high-dimensional optimization problems and offers more stability and robustness. However, if we are only solving for the case of $m=n=3$, then the Newton-Raphson method might be more ideal since this problem is relatively smooth. Adam only requires gradient information and does not involve inverting the Hessian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility function for agent i\n",
    "def utility(x, alpha, omega):\n",
    "    return torch.sum(alpha * (x ** (1 + omega)) / (1 + omega))\n",
    "\n",
    "\n",
    "# Incorporate resource constraints for each good j\n",
    "# Define total endowments\n",
    "endowments = torch.tensor([30.0, 30.0, 30.0], dtype = torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case I: Homogeneous Agents\n",
    "\n",
    "> $m = n = 3$\n",
    "\n",
    "All agents $j$ have the same parameters $\\alpha_j, \\omega_j^i$ and social weights $\\lambda_j$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Social planner's objective, with Homogeneous Agents\n",
    "def social_planner_objective(x, alpha_j, omega_j, lambda_i, endowments, penalty_weight = 1000):\n",
    "    total_utility = 0\n",
    "    total_allocations = torch.zeros(m)\n",
    "\n",
    "    for i in range(n):\n",
    "        x_i_softplus = torch.nn.functional.softplus(x[i]) # ensure nonneg allocations\n",
    "        total_utility += lambda_i[i] * utility(x_i_softplus, alpha_j, omega_j)\n",
    "        total_allocations += x_i_softplus\n",
    "\n",
    "    # penalize if RC is violated, i.e. total allocations > endowments\n",
    "    penalty = torch.sum(torch.clamp(total_allocations - endowments, min = 0) ** 2)\n",
    "\n",
    "    return -total_utility + penalty_weight * penalty   # maximization, so take negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 99: Objective = 45.15373229980469\n",
      "Iteration 199: Objective = 93.82718658447266\n",
      "Iteration 299: Objective = 161.97462463378906\n",
      "Iteration 399: Objective = 243.71197509765625\n",
      "Iteration 499: Objective = 335.66778564453125\n",
      "Iteration 599: Objective = 436.1275634765625\n",
      "Iteration 699: Objective = 544.0576171875\n",
      "Iteration 799: Objective = 569.2916870117188\n",
      "Iteration 899: Objective = 569.2935791015625\n",
      "Iteration 999: Objective = 569.2957763671875\n",
      "\n",
      " Optimal allocations for Homogeneous Agents:\n",
      "            Good 1     Good 2     Good 3\n",
      "Agent 1   9.866802   9.997384  10.040535\n",
      "Agent 2  10.101438   9.863719  10.038213\n",
      "Agent 3  10.036505  10.143645   9.925999\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# =============== DEFINE PARAMETERS ===============\n",
    "m, n = 3, 3  # 3 goods, 3 agents\n",
    "alpha_j = torch.tensor([1.0, 1.0, 1.0], dtype = torch.float32) \n",
    "omega_j = torch.tensor([[0.5, 0.5, 0.5]] * n, dtype = torch.float32)\n",
    "lambda_i = torch.tensor([1.0, 1.0, 1.0], dtype = torch.float32)  # Pareto weights\n",
    "\n",
    "# initial allocations (using small positive values)\n",
    "x_i = torch.rand((n, m), requires_grad = True)\n",
    "\n",
    "# Set up optimizer\n",
    "optimizer = optim.Adam([x_i], lr = 0.01)\n",
    "\n",
    "# Optimization\n",
    "num_iterations = 1000\n",
    "for iteration in range(num_iterations):\n",
    "    optimizer.zero_grad()\n",
    "    objective = social_planner_objective(x_i, alpha_j, omega_j, lambda_i, endowments)\n",
    "    objective.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (iteration+1) % 100 == 0:\n",
    "        print(f\"Iteration {iteration}: Objective = {-objective.item()}\")\n",
    "\n",
    "\n",
    "# Final optimal allocations\n",
    "final_allocations = torch.nn.functional.softplus(x_i).detach().numpy()\n",
    "\n",
    "# print(\"Total allocations per good:\")\n",
    "# print(final_allocations.sum(axis=0))\n",
    "# print(\"Endowments:\")\n",
    "# print(endowments.numpy())\n",
    "\n",
    "df_final_homog = pd.DataFrame(\n",
    "    final_allocations,\n",
    "    index = [f'Agent {i+1}' for i in range(final_allocations.shape[0])],\n",
    "    columns = [f'Good {j+1}' for j in range(final_allocations.shape[1])]\n",
    ")\n",
    "\n",
    "print(\"\\n Optimal allocations for Homogeneous Agents:\")\n",
    "print(df_final_homog)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case II: Heterogeneous Agents \n",
    "> $m = n = 3$\n",
    "\n",
    "* Each agent $i$ has their own set of $\\alpha_j, \\omega_j^i$ parameters.\n",
    "* Pareto weights $\\lambda_i$ differ among agents.\n",
    "* Resource constraint remains the same. Total endowment for each good is 30."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Social planner's objective, with Heterogeneous Agents\n",
    "\n",
    "def social_planner_objective(x, alpha_j, omega_j, lambda_i, endowments, penalty_weight = 1000):\n",
    "    total_utility = 0\n",
    "    total_allocations = torch.zeros(m)\n",
    "\n",
    "    for i in range(n):\n",
    "        x_i_softplus = torch.nn.functional.softplus(x[i]) # ensure nonneg allocations\n",
    "        total_utility += lambda_i[i] * utility(x_i_softplus, alpha_j[i], omega_j[i])\n",
    "        total_allocations += x_i_softplus\n",
    "\n",
    "    # penalize if RC is violated, i.e. total allocations > endowments\n",
    "    penalty = torch.sum(torch.clamp(total_allocations - endowments, min = 0) ** 2)\n",
    "\n",
    "    return -total_utility + penalty_weight * penalty   # maximization, so take negative\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 999: Objective = 13.591890335083008\n",
      "Iteration 1999: Objective = 29.935745239257812\n",
      "Iteration 2999: Objective = 51.88038635253906\n",
      "Iteration 3999: Objective = 77.37189483642578\n",
      "Iteration 4999: Objective = 105.61894226074219\n",
      "Iteration 5999: Objective = 136.32730102539062\n",
      "Iteration 6999: Objective = 169.35321044921875\n",
      "Iteration 7999: Objective = 204.5991668701172\n",
      "Iteration 8999: Objective = 220.5120086669922\n",
      "Iteration 9999: Objective = 226.55992126464844\n",
      "\n",
      " Optimal allocations for Heterogeneous Agents:\n",
      "            Good 1     Good 2     Good 3\n",
      "Agent 1   9.150754   9.897849   9.951900\n",
      "Agent 2  10.499072  11.412535  11.162815\n",
      "Agent 3  10.351420   8.690802   8.888128\n"
     ]
    }
   ],
   "source": [
    "# =============== DEFINE PARAMETERS ===============\n",
    "alpha_j = torch.tensor([[1.0, 0.8, 1.2],  # agent 1\n",
    "                        [1.1, 0.9, 1.3],  # agent 2\n",
    "                        [0.9, 0.7, 1.1]])  # agent 3\n",
    "\n",
    "omega_j = torch.tensor([[0.3, 0.5, 0.7], \n",
    "                        [0.4, 0.6, 0.8],  \n",
    "                        [0.5, 0.4, 0.6]]) \n",
    "\n",
    "lambda_i = torch.tensor([0.9, 1.1, 1.0])  # Social weights\n",
    "\n",
    "\n",
    "# initial allocations (using small positive values)\n",
    "x_i = torch.rand((n, m), requires_grad = True)\n",
    "#x_i = (torch.rand((n, m), requires_grad=True) * endowments / n).clone().detach().requires_grad_(True)\n",
    "\n",
    "# Set up optimizer\n",
    "optimizer = optim.Adam([x_i], lr = 0.001)\n",
    "\n",
    "# Optimization\n",
    "num_iterations = 10000\n",
    "for iteration in range(num_iterations):\n",
    "    optimizer.zero_grad()\n",
    "    objective = social_planner_objective(x_i, alpha_j, omega_j, lambda_i, endowments)\n",
    "    objective.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (iteration+1) % 1000 == 0:\n",
    "        print(f\"Iteration {iteration}: Objective = {-objective.item()}\")\n",
    "\n",
    "\n",
    "# Final optimal allocations\n",
    "final_allocations = torch.nn.functional.softplus(x_i).detach().numpy()\n",
    "\n",
    "# print(\"Total allocations per good:\")\n",
    "# print(final_allocations.sum(axis=0))\n",
    "# print(\"Endowments:\")\n",
    "# print(endowments.numpy())\n",
    "\n",
    "df_final_heterog = pd.DataFrame(\n",
    "    final_allocations,\n",
    "    index = [f'Agent {i+1}' for i in range(final_allocations.shape[0])],\n",
    "    columns = [f'Good {j+1}' for j in range(final_allocations.shape[1])]\n",
    ")\n",
    "\n",
    "print(\"\\n Optimal allocations for Heterogeneous Agents:\")\n",
    "print(df_final_heterog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
