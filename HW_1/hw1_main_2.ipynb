{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Econ 8210 Quant Macro, Homework 1\n",
    "## Part 2 - Solution Methods\n",
    "Haosi Shen, Fall 2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Housekeeping\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import torch\n",
    "\n",
    "np.random.seed(42) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing Pareto Efficient Allocations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider an endowment economy with $m$ different goods and $n$ agents. Each agent $i = 1, ..., n$ has an endowment $e_j^i >0$ for every $j = 1, ..., m$ and a utility function of the form\n",
    "$$ u^i (x) = \\sum_{j=1}^{m} \\alpha_j \\frac{x_{j}^{1+\\omega_j^i}}{1+\\omega_j^i} $$\n",
    "where $\\alpha_j > 0 > \\omega_j^i$ are agent-specific parameters.\n",
    "\n",
    "Given some social weights $\\lambda_i > 0$, solve for the social plannerâ€™s problem for $m = n = 3$ using the **Adam (Adaptive Moment Estimation)** method. Try different values of $\\alpha_j,\\;\\omega_j^i,\\;\\lambda_i$. \n",
    "\n",
    "$$ \\max_{\\{x^i\\}} \\; \\sum_{i=1}^{n} \\lambda_i u^{i}(x)$$\n",
    "\n",
    "Compute first the case where all the agents have the same parameters and\n",
    "social weights and later a case where there is a fair degree of heterogeneity.\n",
    "\n",
    "How does the method perform? How does heterogeneity in the agent-specific parameters\n",
    "affect the results?\n",
    "\n",
    "Can you handle the case where $m = n = 10$?\n",
    "\n",
    "> I choose to use **Adam** since it is more efficient for high-dimensional optimization problems and offers more stability and robustness. However, if we are only solving for the case of $m=n=3$, then the Newton-Raphson method might be more ideal since this problem is relatively smooth. Adam only requires gradient information and does not involve inverting the Hessian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility function for agent i\n",
    "def utility(x, alpha, omega):\n",
    "    return torch.sum(alpha * (x ** (1 + omega)) / (1 + omega))\n",
    "\n",
    "\n",
    "# Incorporate resource constraints for each good j\n",
    "# Define total endowments\n",
    "endowments = torch.tensor([30.0, 30.0, 30.0], dtype = torch.float32)\n",
    "# endowments = torch.tensor([10.0, 20.0, 40.0], dtype = torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case I: Homogeneous Agents\n",
    "\n",
    "> $m = n = 3$\n",
    "\n",
    "All agents $j$ have the same parameters $\\alpha_j, \\omega_j^i$ and social weights $\\lambda_j$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Social planner's objective, with Homogeneous Agents\n",
    "def social_planner_objective_homog(x, alpha_j, omega_j, lambda_i, endowments, penalty_weight = 1000):\n",
    "    total_utility = 0\n",
    "    total_allocations = torch.zeros(m)\n",
    "\n",
    "    for i in range(n):\n",
    "        x_i_softplus = torch.nn.functional.softplus(x[i]) # ensure nonneg allocations\n",
    "        total_utility += lambda_i[i] * utility(x_i_softplus, alpha_j, omega_j)\n",
    "        total_allocations += x_i_softplus\n",
    "\n",
    "    # penalize if RC is violated, i.e. total allocations > endowments\n",
    "    penalty = torch.sum(torch.clamp(total_allocations - endowments, min = 0) ** 2)\n",
    "\n",
    "    return -total_utility + penalty_weight * penalty   # maximization, so take negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 99: Objective = 40.35831069946289\n",
      "Iteration 199: Objective = 88.05348205566406\n",
      "Iteration 299: Objective = 156.2135772705078\n",
      "Iteration 399: Objective = 238.33685302734375\n",
      "Iteration 499: Objective = 330.75048828125\n",
      "Iteration 599: Objective = 431.65130615234375\n",
      "Iteration 699: Objective = 539.98388671875\n",
      "Iteration 799: Objective = 569.2763061523438\n",
      "Iteration 899: Objective = 569.2817993164062\n",
      "Iteration 999: Objective = 569.2822265625\n",
      "\n",
      " Optimal allocations for Homogeneous Agents:\n",
      "            Good 1     Good 2     Good 3\n",
      "Agent 1  10.044940   9.975189  10.050016\n",
      "Agent 2   9.990323   9.937491   9.998878\n",
      "Agent 3   9.969482  10.092060   9.955850\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# =============== DEFINE PARAMETERS ===============\n",
    "m, n = 3, 3  # 3 goods, 3 agents\n",
    "alpha_j = torch.tensor([1.0, 1.0, 1.0], dtype = torch.float32) \n",
    "omega_j = torch.tensor([[0.5, 0.5, 0.5]] * n, dtype = torch.float32)\n",
    "lambda_i = torch.tensor([1.0, 1.0, 1.0], dtype = torch.float32)  # Pareto weights\n",
    "\n",
    "# initial allocations (using small positive values)\n",
    "x_i = torch.rand((n, m), requires_grad = True)\n",
    "\n",
    "# Set up optimizer\n",
    "optimizer = optim.Adam([x_i], lr = 0.01)\n",
    "\n",
    "# Optimization\n",
    "num_iterations = 1000\n",
    "for iteration in range(num_iterations):\n",
    "    optimizer.zero_grad()\n",
    "    objective = social_planner_objective_homog(x_i, alpha_j, omega_j, lambda_i, endowments)\n",
    "    objective.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (iteration+1) % 100 == 0:\n",
    "        print(f\"Iteration {iteration}: Objective = {-objective.item()}\")\n",
    "\n",
    "\n",
    "# Final optimal allocations\n",
    "final_allocations = torch.nn.functional.softplus(x_i).detach().numpy()\n",
    "\n",
    "df_final_homog = pd.DataFrame(\n",
    "    final_allocations,\n",
    "    index = [f'Agent {i+1}' for i in range(final_allocations.shape[0])],\n",
    "    columns = [f'Good {j+1}' for j in range(final_allocations.shape[1])]\n",
    ")\n",
    "\n",
    "print(\"\\n Optimal allocations for Homogeneous Agents:\")\n",
    "print(df_final_homog)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case II: Heterogeneous Agents \n",
    "> $m = n = 3$\n",
    "\n",
    "* Each agent $i$ has their own set of $\\alpha_j, \\omega_j^i$ parameters.\n",
    "* Pareto weights $\\lambda_i$ differ among agents.\n",
    "* Resource constraint remains the same. Total endowment for each good is 30."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Social planner's objective, with Heterogeneous Agents\n",
    "\n",
    "def social_planner_objective_heterog(x, alpha_j, omega_j, lambda_i, endowments, penalty_weight = 1000):\n",
    "    total_utility = 0\n",
    "    total_allocations = torch.zeros(m)\n",
    "\n",
    "    for i in range(n):\n",
    "        x_i_softplus = torch.nn.functional.softplus(x[i]) # ensure nonneg allocations\n",
    "        total_utility += lambda_i[i] * utility(x_i_softplus, alpha_j[i], omega_j[i])\n",
    "        total_allocations += x_i_softplus\n",
    "\n",
    "    # penalize if RC is violated, i.e. total allocations > endowments\n",
    "    penalty = torch.sum(torch.clamp(total_allocations - endowments, min = 0) ** 2)\n",
    "\n",
    "    return -total_utility + penalty_weight * penalty   # maximization, so take negative\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 999: Objective = 16.520526885986328\n",
      "Iteration 1999: Objective = 33.85341262817383\n",
      "Iteration 2999: Objective = 56.39704513549805\n",
      "Iteration 3999: Objective = 82.38998413085938\n",
      "Iteration 4999: Objective = 111.12158966064453\n",
      "Iteration 5999: Objective = 142.30734252929688\n",
      "Iteration 6999: Objective = 175.8013916015625\n",
      "Iteration 7999: Objective = 210.39227294921875\n",
      "Iteration 8999: Objective = 220.77528381347656\n",
      "Iteration 9999: Objective = 227.41481018066406\n",
      "\n",
      " Optimal allocations for Heterogeneous Agents:\n",
      "            Good 1     Good 2     Good 3\n",
      "Agent 1   9.294359  10.029044   9.890506\n",
      "Agent 2  10.500991  11.101754  11.470001\n",
      "Agent 3  10.205898   8.870392   8.642317\n"
     ]
    }
   ],
   "source": [
    "# =============== DEFINE PARAMETERS ===============\n",
    "alpha_j = torch.tensor([[1.0, 0.8, 1.2],  # agent 1\n",
    "                        [1.1, 0.9, 1.3],  # agent 2\n",
    "                        [0.9, 0.7, 1.1]])  # agent 3\n",
    "\n",
    "omega_j = torch.tensor([[0.3, 0.5, 0.7], \n",
    "                        [0.4, 0.6, 0.8],  \n",
    "                        [0.5, 0.4, 0.6]]) \n",
    "\n",
    "lambda_i = torch.tensor([0.9, 1.1, 1.0])  # Social weights\n",
    "\n",
    "\n",
    "# initial allocations (using small positive values)\n",
    "x_i = torch.rand((n, m), requires_grad = True)\n",
    "#x_i = (torch.rand((n, m), requires_grad=True) * endowments / n).clone().detach().requires_grad_(True)\n",
    "\n",
    "# Set up optimizer\n",
    "optimizer = optim.Adam([x_i], lr = 0.001)\n",
    "\n",
    "# Optimization\n",
    "num_iterations = 10000\n",
    "for iteration in range(num_iterations):\n",
    "    optimizer.zero_grad()\n",
    "    objective = social_planner_objective_heterog(x_i, alpha_j, omega_j, lambda_i, endowments)\n",
    "    objective.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (iteration+1) % 1000 == 0:\n",
    "        print(f\"Iteration {iteration}: Objective = {-objective.item()}\")\n",
    "\n",
    "\n",
    "# Final optimal allocations\n",
    "final_allocations = torch.nn.functional.softplus(x_i).detach().numpy()\n",
    "\n",
    "df_final_heterog = pd.DataFrame(\n",
    "    final_allocations,\n",
    "    index = [f'Agent {i+1}' for i in range(final_allocations.shape[0])],\n",
    "    columns = [f'Good {j+1}' for j in range(final_allocations.shape[1])]\n",
    ")\n",
    "\n",
    "print(\"\\n Optimal allocations for Heterogeneous Agents:\")\n",
    "print(df_final_heterog)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The `Adam` optimizer performs fairly well for both the homogeneous and heterogeneous agents cases. The computations are fast and produce sufficiently accurate results. \n",
    "> \n",
    "> Introducing heterogeneity in the agent-specific parameters does affect the socially optimal allocations, and significantly slow down convergence of optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case III: 10 Agents, 10 Goods\n",
    "\n",
    "> $m=n=10$\n",
    "\n",
    "Since the `Adam` optimizer generally works well for higher-dimension problems, we now try computing the Pareto efficient allocations of a 10-agent 10-good economy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Consider homogeneity across agents.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 99: Objective = 154.7710723876953\n",
      "Iteration 199: Objective = 332.29345703125\n",
      "Iteration 299: Objective = 345.7464599609375\n",
      "Iteration 399: Objective = 347.0973205566406\n",
      "Iteration 499: Objective = 347.120361328125\n",
      "Iteration 599: Objective = 347.1561584472656\n",
      "Iteration 699: Objective = 346.99609375\n",
      "Iteration 799: Objective = 346.80059814453125\n",
      "Iteration 899: Objective = 347.25238037109375\n",
      "Iteration 999: Objective = 346.9298095703125\n",
      "\n",
      " Optimal allocations for Homogeneous Agents:\n",
      "            Good 1    Good 2    Good 3    Good 4    Good 5    Good 6  \\\n",
      "Agent 1   2.914130  3.263182  2.595877  2.866040  2.661801  3.439897   \n",
      "Agent 2   3.327475  3.326041  2.884481  3.391286  3.429351  2.896719   \n",
      "Agent 3   2.730921  3.062123  3.117459  2.966691  3.372595  2.932783   \n",
      "Agent 4   2.764751  3.115304  3.232926  3.463181  2.797698  3.470091   \n",
      "Agent 5   2.794737  2.827952  3.373684  2.926597  2.746746  2.615396   \n",
      "Agent 6   3.634638  3.031342  3.408268  2.973178  2.690806  2.633177   \n",
      "Agent 7   3.136706  2.847705  3.328062  2.657247  3.289093  2.668346   \n",
      "Agent 8   2.797498  3.040924  2.944327  2.913477  3.467157  2.644832   \n",
      "Agent 9   2.826400  2.623460  2.658698  3.062328  2.647918  3.432707   \n",
      "Agent 10  3.044721  2.670045  2.456552  2.780630  2.821578  3.266935   \n",
      "\n",
      "            Good 7    Good 8    Good 9   Good 10  \n",
      "Agent 1   3.523233  2.693148  3.114190  2.429691  \n",
      "Agent 2   2.717145  2.831064  3.278423  3.177675  \n",
      "Agent 3   3.193230  2.732934  3.504444  3.124863  \n",
      "Agent 4   3.444844  3.639624  3.015821  3.165783  \n",
      "Agent 5   2.504086  3.203875  2.776836  2.701004  \n",
      "Agent 6   2.543962  2.743759  3.126724  3.234079  \n",
      "Agent 7   2.540202  2.877006  2.881391  2.793743  \n",
      "Agent 8   2.952490  3.228734  2.806731  3.372881  \n",
      "Agent 9   3.395864  3.237501  3.005459  3.454474  \n",
      "Agent 10  3.138696  2.798649  2.448050  2.456301  \n"
     ]
    }
   ],
   "source": [
    "m, n = 10, 10  # 10 goods, 10 agents\n",
    "endowments = torch.tensor([30.0] * m)  # total endowments for 10 goods\n",
    "\n",
    "alpha_j = torch.tensor([1.0] * m)  \n",
    "omega_j = torch.tensor([0.5] * m)  \n",
    "lambda_i = torch.tensor([1.0] * n)\n",
    "\n",
    "x_i = torch.rand((n, m), requires_grad = True)  # initial allocations\n",
    "\n",
    "optimizer = optim.Adam([x_i], lr = 0.01)\n",
    "\n",
    "# Optimization\n",
    "num_iterations = 1000\n",
    "for iteration in range(num_iterations):\n",
    "    optimizer.zero_grad()\n",
    "    objective = social_planner_objective_homog(x_i, alpha_j, omega_j, lambda_i, endowments)\n",
    "    objective.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (iteration+1) % 100 == 0:\n",
    "        print(f\"Iteration {iteration}: Objective = {-objective.item()}\")\n",
    "\n",
    "\n",
    "# Final optimal allocations\n",
    "final_allocations = torch.nn.functional.softplus(x_i).detach().numpy()\n",
    "\n",
    "df_final_homog = pd.DataFrame(\n",
    "    final_allocations,\n",
    "    index = [f'Agent {i+1}' for i in range(final_allocations.shape[0])],\n",
    "    columns = [f'Good {j+1}' for j in range(final_allocations.shape[1])]\n",
    ")\n",
    "\n",
    "print(\"\\n Optimal allocations for Homogeneous Agents:\")\n",
    "print(df_final_homog)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now, try the heterogeneous agents case.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heterogeneous agent-specific parameters\n",
    "alpha_j = torch.tensor([\n",
    "    [1.0, 0.8, 1.2, 0.9, 1.1, 0.7, 1.3, 0.95, 1.05, 1.15] for _ in range(n)]) \n",
    "\n",
    "omega_j = torch.tensor([\n",
    "    [0.3, 0.4, 0.5, 0.35, 0.45, 0.55, 0.25, 0.6, 0.65, 0.5] for _ in range(n)])\n",
    "\n",
    "lambda_i = torch.tensor([0.9, 1.1, 0.8, 1.2, 1.0, 0.85, 1.15, 0.95, 1.05, 1.0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 999: Objective = 169.78448486328125\n",
      "Iteration 1999: Objective = 333.05609130859375\n",
      "Iteration 2999: Objective = 349.08544921875\n",
      "Iteration 3999: Objective = 357.8846740722656\n",
      "Iteration 4999: Objective = 380.14593505859375\n",
      "Iteration 5999: Objective = 423.39337158203125\n",
      "Iteration 6999: Objective = 482.5692138671875\n",
      "Iteration 7999: Objective = 535.2533569335938\n",
      "Iteration 8999: Objective = 575.578857421875\n",
      "Iteration 9999: Objective = 611.5863647460938\n",
      "\n",
      " Optimal allocations for Heterogeneous Agents:\n",
      "            Good 1    Good 2    Good 3    Good 4    Good 5    Good 6  \\\n",
      "Agent 1   0.117886  0.152617  0.065931  0.127449  0.081844  0.062376   \n",
      "Agent 2   7.421418  7.486674  7.738941  8.932431  8.769207  8.761271   \n",
      "Agent 3   0.107674  0.107837  0.043850  0.093451  0.042553  0.059907   \n",
      "Agent 4   8.832698  9.452785  9.899761  8.762597  9.390129  9.841595   \n",
      "Agent 5   0.428526  0.206919  0.275376  0.326674  0.179943  0.065967   \n",
      "Agent 6   0.168618  0.161534  0.052743  0.141985  0.047812  0.058501   \n",
      "Agent 7   8.306960  8.029416  9.459732  8.559828  9.491012  5.732125   \n",
      "Agent 8   0.125130  0.237292  0.080057  0.807765  0.064440  0.197890   \n",
      "Agent 9   4.206989  3.868118  2.296957  1.955772  1.755545  5.112945   \n",
      "Agent 10  0.285040  0.297688  0.088499  0.292868  0.179082  0.108503   \n",
      "\n",
      "            Good 7    Good 8    Good 9   Good 10  \n",
      "Agent 1   0.103891  0.081769  0.043904  0.078481  \n",
      "Agent 2   6.755013  4.863229  8.375223  7.083513  \n",
      "Agent 3   0.072823  0.060816  0.057218  0.062300  \n",
      "Agent 4   8.811281  9.209170  9.565404  8.927081  \n",
      "Agent 5   0.368880  0.224203  0.095029  0.155650  \n",
      "Agent 6   0.145475  0.052755  0.084592  0.055442  \n",
      "Agent 7   7.995642  8.663912  9.418411  8.978868  \n",
      "Agent 8   0.356315  0.070192  0.086048  0.078644  \n",
      "Agent 9   4.374548  6.669853  2.203182  4.503004  \n",
      "Agent 10  1.017202  0.105728  0.073280  0.078731  \n"
     ]
    }
   ],
   "source": [
    "x_i = torch.rand((n, m), requires_grad=True)\n",
    "optimizer = optim.Adam([x_i], lr = 0.001)\n",
    "\n",
    "# Optimization\n",
    "num_iterations = 10000\n",
    "for iteration in range(num_iterations):\n",
    "    optimizer.zero_grad()\n",
    "    objective = social_planner_objective_heterog(x_i, alpha_j, omega_j, lambda_i, endowments)\n",
    "    objective.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (iteration+1) % 1000 == 0:\n",
    "        print(f\"Iteration {iteration}: Objective = {-objective.item()}\")\n",
    "\n",
    "\n",
    "# Final optimal allocations\n",
    "final_allocations = torch.nn.functional.softplus(x_i).detach().numpy()\n",
    "\n",
    "df_final_heterog = pd.DataFrame(\n",
    "    final_allocations,\n",
    "    index = [f'Agent {i+1}' for i in range(final_allocations.shape[0])],\n",
    "    columns = [f'Good {j+1}' for j in range(final_allocations.shape[1])]\n",
    ")\n",
    "\n",
    "print(\"\\n Optimal allocations for Heterogeneous Agents:\")\n",
    "print(df_final_heterog)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The method works well for $m=n=10$, even after we introduce household heterogeneity. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing Equilibrium Allocations\n",
    "\n",
    "Using the same model as in the previous exercise. Find the equilibrium prices for each good $p^j$.\n",
    "\n",
    "1. Solve for the first-order conditions of each agent.\n",
    "1. Aggregate the excess demands. \n",
    "1. Solve the resulting system of nonlinear equations, when all markets clear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Homogeneous Agents, $m=n=3$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FOCs**\n",
    "> To find the demand for each good, we solve the Lagrangean:\n",
    "> $$\\mathcal{L}=\\sum_{j=1}^{m}\\alpha_j \\cdot \\frac{x_{ij}^{1+\\omega}}{1+\\omega}-\\lambda_i \\Big(\\sum_{j=1}^{m}p_j\\cdot x_{ij} - Y_i\\Big)$$\n",
    "> where $Y_i$ is the income of agent $i$. \n",
    "> The FOCs for this problem yield\n",
    "> $$ x_{ij} = \\frac{\\alpha_j \\cdot p_{j}^{-\\omega}}{\\sum_{k=1}^{m} \\alpha_k \\cdot p_{k}^{-\\omega} }\\cdot Y_i $$\n",
    "\n",
    "**Excess Demand**\n",
    "> Aggregate the demand across agents and compare it to the total endowment to find the excess demand for each good\n",
    "> $$z_j(p) =  \\sum_{i=1}^{n} x_{ij}(p) - e_j$$\n",
    "> Markets clear when $z_j(p)=0$ for all goods $j$.\n",
    "\n",
    "**Equilibrium Prices**\n",
    "> The goal is to find prices $p$ such that the excess demand is zero for all goods. This involves numerically solving a system of nonlinear equations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import fsolve\n",
    "\n",
    "# Step 1: Solve for individual agent's demand from FOC\n",
    "\n",
    "def agent_demand(p, alpha, omega, income):\n",
    "    \"\"\"\n",
    "    - p: prices for each good (vector of size m)\n",
    "    - alpha: preference weights (vector of size m)\n",
    "    - omega: curvature parameter (scalar)\n",
    "    - income: agent's income (scalar)\n",
    "\n",
    "    Returns: demand for each good (vector of size m)\n",
    "    \"\"\"\n",
    "    return (alpha * (p ** -omega)) / (np.sum(alpha * (p ** -omega))) * income\n",
    "\n",
    "\n",
    "\n",
    "# Step 2: Solve for excess demand for each good\n",
    "\n",
    "def excess_demand(p, endowments, alpha, omega, incomes):\n",
    "    \"\"\"\n",
    "    - endowments: total endowment for each good (vector of size m)\n",
    "    - incomes: total income for each agent (vector of size n)\n",
    "\n",
    "    Returns: excess demand for each good (vector of size m)\n",
    "    \"\"\"\n",
    "    n, m = incomes.shape[0], endowments.shape[0]\n",
    "    total_demand = np.zeros(m)\n",
    "    \n",
    "    for i in range(n):\n",
    "        demand_i = agent_demand(p, alpha, omega, incomes[i])\n",
    "        total_demand += demand_i\n",
    "    \n",
    "    return total_demand - endowments \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Solve for equilibrium prices where excess demand = 0\n",
    "def find_eq_prices(endowments, alpha, omega, incomes):\n",
    "    \"\"\"\n",
    "    Solve for eq prices using `fsolve` to find root of excess demand function.\n",
    "    \"\"\"\n",
    "    p0 = np.ones(endowments.shape[0])   # Initial guess\n",
    "    \n",
    "    eq_prices = fsolve(lambda p: excess_demand(p, endowments, alpha, omega, incomes), p0)\n",
    "    \n",
    "    return eq_prices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "# =============== DEFINE PARAMETERS (homogeneity) ===============\n",
    "m, n = 3, 3  # 3 goods, 3 agents\n",
    "alpha_j = np.ones(m)  # preference weights\n",
    "omega_j = 0.5  # curvature param\n",
    "endowments = np.array([30.0, 30.0, 30.0])  # total endowments\n",
    "incomes = np.ones(n) * np.sum(endowments) / n  # equally distributed income\n",
    "\n",
    "eq_prices = find_eq_prices(endowments, alpha_j, omega_j, incomes)\n",
    "print(eq_prices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By the FWT, every competitive equilibrium is Pareto efficient. Equilibrium prices align with marginal rates of substitution, and budget constraints enforce resource constraints. Therefore, the Arrow-Debreu equilibrium allocations coincide with the Pareto-efficient allocations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heterogeneous Agents, $m=n=3$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FOCs**\n",
    "> To find the demand for each good, we solve the Lagrangean:\n",
    "> $$\\mathcal{L}=\\sum_{j=1}^{m}\\alpha_j^i \\cdot \\frac{x_{ij}^{1+\\omega_j^i}}{1+\\omega_j^i}-\\lambda_i \\Big(\\sum_{j=1}^{m}p_j\\cdot x_{ij} - Y_i\\Big)$$\n",
    "> where $Y_i$ is the income of agent $i$. \n",
    "> The FOCs for this problem yield\n",
    "> $$ x_{ij} = \\frac{\\alpha_j^i \\cdot p_{j}^{-\\omega_j^i}}{\\sum_{k=1}^{m} \\alpha_k^i \\cdot p_{k}^{-\\omega_j^i} }\\cdot Y_i $$\n",
    "\n",
    "**Excess Demand**\n",
    "> Aggregate the demand across agents and compare it to the total endowment to find the excess demand for each good\n",
    "> $$z_j(p) =  \\sum_{i=1}^{n} x_{ij}(p) - e_j$$\n",
    "> Markets clear when $z_j(p)=0$ for all goods $j$.\n",
    "\n",
    "**Equilibrium Prices**\n",
    "> The goal is to find prices $p$ such that the excess demand is zero for all goods. This involves numerically solving a system of nonlinear equations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
