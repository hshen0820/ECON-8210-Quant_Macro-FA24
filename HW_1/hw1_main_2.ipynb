{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Econ 8210 Quant Macro, Homework 1\n",
    "## Part 2 - Solution Methods\n",
    "Haosi Shen, Fall 2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Housekeeping\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import torch\n",
    "\n",
    "np.random.seed(42) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing Pareto Efficient Allocations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider an endowment economy with $m$ different goods and $n$ agents. Each agent $i = 1, ..., n$ has an endowment $e_j^i >0$ for every $j = 1, ..., m$ and a utility function of the form\n",
    "$$ u^i (x) = \\sum_{j=1}^{m} \\alpha_j \\frac{x_{j}^{1+\\omega_j^i}}{1+\\omega_j^i} $$\n",
    "where $\\alpha_j > 0 > \\omega_j^i$ are agent-specific parameters.\n",
    "\n",
    "Given some social weights $\\lambda_i > 0$, solve for the social plannerâ€™s problem for $m = n = 3$ using the **Adam (Adaptive Moment Estimation)** method. Try different values of $\\alpha_j,\\;\\omega_j^i,\\;\\lambda_i$. \n",
    "\n",
    "$$ \\max_{\\{x^i\\}} \\; \\sum_{i=1}^{n} \\lambda_i u^{i}(x)$$\n",
    "\n",
    "Compute first the case where all the agents have the same parameters and\n",
    "social weights and later a case where there is a fair degree of heterogeneity.\n",
    "\n",
    "How does the method perform? How does heterogeneity in the agent-specific parameters\n",
    "affect the results?\n",
    "\n",
    "Can you handle the case where $m = n = 10$?\n",
    "\n",
    "> I choose to use **Adam** since it is more efficient for high-dimensional optimization problems and offers more stability and robustness. However, if we are only solving for the case of $m=n=3$, then the Newton-Raphson method might be more ideal since this problem is relatively smooth. Adam only requires gradient information and does not involve inverting the Hessian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility function for agent i\n",
    "def utility(x, alpha, omega):\n",
    "    return torch.sum(alpha * (x ** (1 + omega)) / (1 + omega))\n",
    "\n",
    "\n",
    "# Incorporate resource constraints for each good j\n",
    "# Define total endowments\n",
    "endowments = torch.tensor([30.0, 30.0, 30.0], dtype = torch.float32)\n",
    "# endowments = torch.tensor([10.0, 20.0, 40.0], dtype = torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case I: Homogeneous Agents\n",
    "\n",
    "> $m = n = 3$\n",
    "\n",
    "All agents $j$ have the same parameters $\\alpha_j, \\omega_j^i$ and social weights $\\lambda_j$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Social planner's objective, with Homogeneous Agents\n",
    "def social_planner_objective_homog(x, alpha_j, omega_j, lambda_i, endowments, penalty_weight = 1000):\n",
    "    total_utility = 0\n",
    "    total_allocations = torch.zeros(m)\n",
    "\n",
    "    for i in range(n):\n",
    "        x_i_softplus = torch.nn.functional.softplus(x[i]) # ensure nonneg allocations\n",
    "        total_utility += lambda_i[i] * utility(x_i_softplus, alpha_j, omega_j)\n",
    "        total_allocations += x_i_softplus\n",
    "\n",
    "    # penalize if RC is violated, i.e. total allocations > endowments\n",
    "    penalty = torch.sum(torch.clamp(total_allocations - endowments, min = 0) ** 2)\n",
    "\n",
    "    return -total_utility + penalty_weight * penalty   # maximization, so take negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 99: Objective = 47.90167236328125\n",
      "Iteration 199: Objective = 97.20240020751953\n",
      "Iteration 299: Objective = 165.30142211914062\n",
      "Iteration 399: Objective = 246.75\n",
      "Iteration 499: Objective = 338.384033203125\n",
      "Iteration 599: Objective = 438.5430908203125\n",
      "Iteration 699: Objective = 546.2037353515625\n",
      "Iteration 799: Objective = 569.2835693359375\n",
      "Iteration 899: Objective = 569.2864379882812\n",
      "Iteration 999: Objective = 569.28759765625\n",
      "\n",
      " Optimal allocations for Homogeneous Agents:\n",
      "            Good 1     Good 2     Good 3\n",
      "Agent 1   9.939378   9.994959   9.863128\n",
      "Agent 2  10.019382   9.975774  10.016639\n",
      "Agent 3  10.045987  10.034011  10.124976\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# =============== DEFINE PARAMETERS ===============\n",
    "m, n = 3, 3  # 3 goods, 3 agents\n",
    "alpha_j = torch.tensor([1.0, 1.0, 1.0], dtype = torch.float32) \n",
    "omega_j = torch.tensor([[0.5, 0.5, 0.5]] * n, dtype = torch.float32)\n",
    "lambda_i = torch.tensor([1.0, 1.0, 1.0], dtype = torch.float32)  # Pareto weights\n",
    "\n",
    "# initial allocations (using small positive values)\n",
    "x_i = torch.rand((n, m), requires_grad = True)\n",
    "\n",
    "# Set up optimizer\n",
    "optimizer = optim.Adam([x_i], lr = 0.01)\n",
    "\n",
    "# Optimization\n",
    "num_iterations = 1000\n",
    "for iteration in range(num_iterations):\n",
    "    optimizer.zero_grad()\n",
    "    objective = social_planner_objective_homog(x_i, alpha_j, omega_j, lambda_i, endowments)\n",
    "    objective.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (iteration+1) % 100 == 0:\n",
    "        print(f\"Iteration {iteration}: Objective = {-objective.item()}\")\n",
    "\n",
    "\n",
    "# Final optimal allocations\n",
    "final_allocations = torch.nn.functional.softplus(x_i).detach().numpy()\n",
    "\n",
    "df_final_homog = pd.DataFrame(\n",
    "    final_allocations,\n",
    "    index = [f'Agent {i+1}' for i in range(final_allocations.shape[0])],\n",
    "    columns = [f'Good {j+1}' for j in range(final_allocations.shape[1])]\n",
    ")\n",
    "\n",
    "print(\"\\n Optimal allocations for Homogeneous Agents:\")\n",
    "print(df_final_homog)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case II: Heterogeneous Agents \n",
    "> $m = n = 3$\n",
    "\n",
    "* Each agent $i$ has their own set of $\\alpha_j, \\omega_j^i$ parameters.\n",
    "* Pareto weights $\\lambda_i$ differ among agents.\n",
    "* Resource constraint remains the same. Total endowment for each good is 30."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Social planner's objective, with Heterogeneous Agents\n",
    "\n",
    "def social_planner_objective_heterog(x, alpha_j, omega_j, lambda_i, endowments, penalty_weight = 1000):\n",
    "    total_utility = 0\n",
    "    total_allocations = torch.zeros(m)\n",
    "\n",
    "    for i in range(n):\n",
    "        x_i_softplus = torch.nn.functional.softplus(x[i]) # ensure nonneg allocations\n",
    "        total_utility += lambda_i[i] * utility(x_i_softplus, alpha_j[i], omega_j[i])\n",
    "        total_allocations += x_i_softplus\n",
    "\n",
    "    # penalize if RC is violated, i.e. total allocations > endowments\n",
    "    penalty = torch.sum(torch.clamp(total_allocations - endowments, min = 0) ** 2)\n",
    "\n",
    "    return -total_utility + penalty_weight * penalty   # maximization, so take negative\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 999: Objective = 15.176987648010254\n",
      "Iteration 1999: Objective = 32.06589126586914\n",
      "Iteration 2999: Objective = 54.335941314697266\n",
      "Iteration 3999: Objective = 80.09893798828125\n",
      "Iteration 4999: Objective = 108.60932922363281\n",
      "Iteration 5999: Objective = 139.5780487060547\n",
      "Iteration 6999: Objective = 172.86007690429688\n",
      "Iteration 7999: Objective = 208.35690307617188\n",
      "Iteration 8999: Objective = 221.42538452148438\n",
      "Iteration 9999: Objective = 227.85813903808594\n",
      "\n",
      " Optimal allocations for Heterogeneous Agents:\n",
      "            Good 1     Good 2     Good 3\n",
      "Agent 1   9.087026   9.912905   9.912929\n",
      "Agent 2  10.720445  11.217888  11.453905\n",
      "Agent 3  10.193778   8.870395   8.635993\n"
     ]
    }
   ],
   "source": [
    "# =============== DEFINE PARAMETERS ===============\n",
    "alpha_j = torch.tensor([[1.0, 0.8, 1.2],  # agent 1\n",
    "                        [1.1, 0.9, 1.3],  # agent 2\n",
    "                        [0.9, 0.7, 1.1]])  # agent 3\n",
    "\n",
    "omega_j = torch.tensor([[0.3, 0.5, 0.7], \n",
    "                        [0.4, 0.6, 0.8],  \n",
    "                        [0.5, 0.4, 0.6]]) \n",
    "\n",
    "lambda_i = torch.tensor([0.9, 1.1, 1.0])  # Social weights\n",
    "\n",
    "\n",
    "# initial allocations (using small positive values)\n",
    "x_i = torch.rand((n, m), requires_grad = True)\n",
    "#x_i = (torch.rand((n, m), requires_grad=True) * endowments / n).clone().detach().requires_grad_(True)\n",
    "\n",
    "# Set up optimizer\n",
    "optimizer = optim.Adam([x_i], lr = 0.001)\n",
    "\n",
    "# Optimization\n",
    "num_iterations = 10000\n",
    "for iteration in range(num_iterations):\n",
    "    optimizer.zero_grad()\n",
    "    objective = social_planner_objective_heterog(x_i, alpha_j, omega_j, lambda_i, endowments)\n",
    "    objective.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (iteration+1) % 1000 == 0:\n",
    "        print(f\"Iteration {iteration}: Objective = {-objective.item()}\")\n",
    "\n",
    "\n",
    "# Final optimal allocations\n",
    "final_allocations = torch.nn.functional.softplus(x_i).detach().numpy()\n",
    "\n",
    "df_final_heterog = pd.DataFrame(\n",
    "    final_allocations,\n",
    "    index = [f'Agent {i+1}' for i in range(final_allocations.shape[0])],\n",
    "    columns = [f'Good {j+1}' for j in range(final_allocations.shape[1])]\n",
    ")\n",
    "\n",
    "print(\"\\n Optimal allocations for Heterogeneous Agents:\")\n",
    "print(df_final_heterog)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The `Adam` optimizer performs fairly well for both the homogeneous and heterogeneous agents cases. The computations are fast and produce sufficiently accurate results. \n",
    "> \n",
    "> Introducing heterogeneity in the agent-specific parameters does affect the socially optimal allocations, and significantly slow down convergence of optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case III: 10 Agents, 10 Goods\n",
    "\n",
    "> $m=n=10$\n",
    "\n",
    "Since the `Adam` optimizer generally works well for higher-dimension problems, we now try computing the Pareto efficient allocations of a 10-agent 10-good economy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Consider homogeneity across agents.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 99: Objective = 162.29444885253906\n",
      "Iteration 199: Objective = 338.72503662109375\n",
      "Iteration 299: Objective = 346.544677734375\n",
      "Iteration 399: Objective = 346.9432067871094\n",
      "Iteration 499: Objective = 347.1405944824219\n",
      "Iteration 599: Objective = 346.41668701171875\n",
      "Iteration 699: Objective = 347.0489807128906\n",
      "Iteration 799: Objective = 347.0523681640625\n",
      "Iteration 899: Objective = 346.85882568359375\n",
      "Iteration 999: Objective = 347.0088195800781\n",
      "\n",
      " Optimal allocations for Homogeneous Agents:\n",
      "            Good 1    Good 2    Good 3    Good 4    Good 5    Good 6  \\\n",
      "Agent 1   3.332024  3.467447  3.013774  2.921800  3.370348  2.606249   \n",
      "Agent 2   2.523771  3.006770  2.936013  3.410774  3.116770  2.957279   \n",
      "Agent 3   3.396104  3.225600  3.357541  2.552707  2.610854  3.233714   \n",
      "Agent 4   3.280476  2.671251  3.462526  2.876756  3.260174  2.962018   \n",
      "Agent 5   2.772020  3.470001  3.426079  3.285574  3.149989  2.696598   \n",
      "Agent 6   3.035664  2.449300  2.839202  2.810548  3.096043  2.705003   \n",
      "Agent 7   2.777652  2.789296  2.758389  3.008405  2.708738  2.528101   \n",
      "Agent 8   2.705650  2.702750  2.568568  3.027114  2.546056  3.493088   \n",
      "Agent 9   3.028283  3.285958  2.989901  2.703341  3.355656  3.360271   \n",
      "Agent 10  3.133388  2.846654  2.529186  3.403856  2.758366  3.420436   \n",
      "\n",
      "            Good 7    Good 8    Good 9   Good 10  \n",
      "Agent 1   3.204468  2.977697  2.681116  3.469267  \n",
      "Agent 2   3.007643  2.876184  2.976918  3.318517  \n",
      "Agent 3   2.519032  2.946940  3.182735  2.995186  \n",
      "Agent 4   2.975521  2.885053  2.655032  2.490932  \n",
      "Agent 5   2.788965  2.530028  2.645246  2.894739  \n",
      "Agent 6   3.397224  3.412939  3.244644  3.014147  \n",
      "Agent 7   3.386715  2.691527  2.764493  3.504555  \n",
      "Agent 8   2.898546  3.246688  2.857350  3.091565  \n",
      "Agent 9   2.660341  3.350823  3.496938  2.711888  \n",
      "Agent 10  3.143730  3.055660  3.447952  2.470882  \n"
     ]
    }
   ],
   "source": [
    "m, n = 10, 10  # 10 goods, 10 agents\n",
    "endowments = torch.tensor([30.0] * m)  # total endowments for 10 goods\n",
    "\n",
    "alpha_j = torch.tensor([1.0] * m)  \n",
    "omega_j = torch.tensor([0.5] * m)  \n",
    "lambda_i = torch.tensor([1.0] * n)\n",
    "\n",
    "x_i = torch.rand((n, m), requires_grad = True)  # initial allocations\n",
    "\n",
    "optimizer = optim.Adam([x_i], lr = 0.01)\n",
    "\n",
    "# Optimization\n",
    "num_iterations = 1000\n",
    "for iteration in range(num_iterations):\n",
    "    optimizer.zero_grad()\n",
    "    objective = social_planner_objective_homog(x_i, alpha_j, omega_j, lambda_i, endowments)\n",
    "    objective.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (iteration+1) % 100 == 0:\n",
    "        print(f\"Iteration {iteration}: Objective = {-objective.item()}\")\n",
    "\n",
    "\n",
    "# Final optimal allocations\n",
    "final_allocations = torch.nn.functional.softplus(x_i).detach().numpy()\n",
    "\n",
    "df_final_homog = pd.DataFrame(\n",
    "    final_allocations,\n",
    "    index = [f'Agent {i+1}' for i in range(final_allocations.shape[0])],\n",
    "    columns = [f'Good {j+1}' for j in range(final_allocations.shape[1])]\n",
    ")\n",
    "\n",
    "print(\"\\n Optimal allocations for Homogeneous Agents:\")\n",
    "print(df_final_homog)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now, try the heterogeneous agents case.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heterogeneous agent-specific parameters\n",
    "alpha_j = torch.tensor([\n",
    "    [1.0, 0.8, 1.2, 0.9, 1.1, 0.7, 1.3, 0.95, 1.05, 1.15] for _ in range(n)]) \n",
    "\n",
    "omega_j = torch.tensor([\n",
    "    [0.3, 0.4, 0.5, 0.35, 0.45, 0.55, 0.25, 0.6, 0.65, 0.5] for _ in range(n)])\n",
    "\n",
    "lambda_i = torch.tensor([0.9, 1.1, 0.8, 1.2, 1.0, 0.85, 1.15, 0.95, 1.05, 1.0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 999: Objective = 163.5595245361328\n",
      "Iteration 1999: Objective = 327.0325622558594\n",
      "Iteration 2999: Objective = 350.0217590332031\n",
      "Iteration 3999: Objective = 359.7204284667969\n",
      "Iteration 4999: Objective = 383.3825378417969\n",
      "Iteration 5999: Objective = 425.12237548828125\n",
      "Iteration 6999: Objective = 479.7190246582031\n",
      "Iteration 7999: Objective = 534.1912841796875\n",
      "Iteration 8999: Objective = 585.0552978515625\n",
      "Iteration 9999: Objective = 626.8079833984375\n",
      "\n",
      " Optimal allocations for Heterogeneous Agents:\n",
      "            Good 1    Good 2     Good 3    Good 4    Good 5    Good 6  \\\n",
      "Agent 1   0.125988  0.150368   0.046247  0.242932  0.060060  0.066638   \n",
      "Agent 2   6.712726  9.160335   9.289027  6.410796  7.141737  8.688856   \n",
      "Agent 3   0.107602  0.083613   0.053757  0.078370  0.064677  0.084108   \n",
      "Agent 4   8.337194  9.270158  10.074583  8.461432  9.624029  9.113145   \n",
      "Agent 5   1.219632  0.271760   0.103095  0.242591  0.085858  0.106013   \n",
      "Agent 6   0.129397  0.090682   0.056293  0.137855  0.060515  0.114077   \n",
      "Agent 7   8.684693  9.245509   8.882628  7.507277  8.926950  9.008531   \n",
      "Agent 8   0.192096  0.269876   0.076391  0.124505  0.161086  0.204570   \n",
      "Agent 9   2.425830  1.237803   1.057940  5.957640  3.761709  2.503928   \n",
      "Agent 10  2.065659  0.220809   0.361961  0.837475  0.114857  0.111315   \n",
      "\n",
      "            Good 7     Good 8     Good 9   Good 10  \n",
      "Agent 1   0.151070   0.048791   0.043882  0.089763  \n",
      "Agent 2   6.963357   9.351196   0.090432  8.135827  \n",
      "Agent 3   0.067711   0.053213   0.057935  0.050230  \n",
      "Agent 4   8.532018  10.408657  10.121247  9.823153  \n",
      "Agent 5   0.830791   0.425070   1.073013  0.072671  \n",
      "Agent 6   0.131258   0.059712   0.046577  0.044304  \n",
      "Agent 7   8.084649   9.290656  10.193056  8.675082  \n",
      "Agent 8   0.116883   0.166132   0.086491  0.071005  \n",
      "Agent 9   4.879670   0.072214   7.698773  2.929235  \n",
      "Agent 10  0.243674   0.126341   0.590714  0.110523  \n"
     ]
    }
   ],
   "source": [
    "x_i = torch.rand((n, m), requires_grad=True)\n",
    "optimizer = optim.Adam([x_i], lr = 0.001)\n",
    "\n",
    "# Optimization\n",
    "num_iterations = 10000\n",
    "for iteration in range(num_iterations):\n",
    "    optimizer.zero_grad()\n",
    "    objective = social_planner_objective_heterog(x_i, alpha_j, omega_j, lambda_i, endowments)\n",
    "    objective.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (iteration+1) % 1000 == 0:\n",
    "        print(f\"Iteration {iteration}: Objective = {-objective.item()}\")\n",
    "\n",
    "\n",
    "# Final optimal allocations\n",
    "final_allocations = torch.nn.functional.softplus(x_i).detach().numpy()\n",
    "\n",
    "df_final_heterog = pd.DataFrame(\n",
    "    final_allocations,\n",
    "    index = [f'Agent {i+1}' for i in range(final_allocations.shape[0])],\n",
    "    columns = [f'Good {j+1}' for j in range(final_allocations.shape[1])]\n",
    ")\n",
    "\n",
    "print(\"\\n Optimal allocations for Heterogeneous Agents:\")\n",
    "print(df_final_heterog)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The method works well for $m=n=10$, even after we introduce household heterogeneity. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing Equilibrium Allocations\n",
    "\n",
    "Using the same model as in the previous exercise. Find the equilibrium prices for each good $p^j$.\n",
    "\n",
    "1. Solve for the first-order conditions of each agent.\n",
    "1. Aggregate the excess demands. \n",
    "1. Solve the resulting system of nonlinear equations, when all markets clear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Homogeneous Agents, $m=n=3$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FOCs**\n",
    "> To find the demand for each good, we solve the Lagrangean:\n",
    "> $$\\mathcal{L}=\\sum_{j=1}^{m}\\alpha_j \\cdot \\frac{x_{ij}^{1+\\omega}}{1+\\omega}-\\lambda_i \\Big(\\sum_{j=1}^{m}p_j\\cdot x_{ij} - Y_i\\Big)$$\n",
    "> where $Y_i$ is the income of agent $i$. \n",
    "> The FOCs for this problem yield\n",
    "> $$ x_{ij} = \\frac{\\alpha_j \\cdot p_{j}^{-\\omega}}{\\sum_{k=1}^{m} \\alpha_k \\cdot p_{k}^{-\\omega} }\\cdot Y_i $$\n",
    "\n",
    "**Excess Demand**\n",
    "> Aggregate the demand across agents and compare it to the total endowment to find the excess demand for each good\n",
    "> $$z_j(p) =  \\sum_{i=1}^{n} x_{ij}(p) - e_j$$\n",
    "> Markets clear when $z_j(p)=0$ for all goods $j$.\n",
    "\n",
    "**Equilibrium Prices**\n",
    "> The goal is to find prices $p$ such that the excess demand is zero for all goods. This involves numerically solving a system of nonlinear equations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import fsolve\n",
    "\n",
    "# Step 1: Solve for individual agent's demand from FOC\n",
    "\n",
    "def agent_demand(p, alpha, omega, income):\n",
    "    \"\"\"\n",
    "    - p: prices for each good\n",
    "    - alpha: preference weights\n",
    "    - omega: curvature parameter\n",
    "    - income: agent's income\n",
    "\n",
    "    Returns: demand for each good\n",
    "    \"\"\"\n",
    "    return (alpha * (p ** -omega)) / (np.sum(alpha * (p ** -omega))) * income\n",
    "\n",
    "\n",
    "\n",
    "# Step 2: Solve for excess demand for each good\n",
    "\n",
    "def excess_demand(p, endowments, alpha, omega, incomes):\n",
    "    \"\"\"\n",
    "    - endowments: total endowment for each good \n",
    "    - incomes: total income for each agent\n",
    "\n",
    "    Returns: excess demand for each good \n",
    "    \"\"\"\n",
    "    n, m = incomes.shape[0], endowments.shape[0]\n",
    "    total_demand = np.zeros(m)\n",
    "    \n",
    "    for i in range(n):\n",
    "        demand_i = agent_demand(p, alpha, omega, incomes[i])\n",
    "        total_demand += demand_i\n",
    "    \n",
    "    return total_demand - endowments \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Solve for equilibrium prices where excess demand = 0\n",
    "def find_eq_prices(endowments, alpha, omega, incomes):\n",
    "    \"\"\"\n",
    "    Solve for eq prices using `fsolve` to find root of excess demand function.\n",
    "    \"\"\"\n",
    "    p0 = np.ones(endowments.shape[0])   # Initial guess\n",
    "    \n",
    "    eq_prices = fsolve(lambda p: excess_demand(p, endowments, alpha, omega, incomes), p0)\n",
    "    \n",
    "    return eq_prices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Equilibrium Prices for Homogeneous Agents: [1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "# =============== DEFINE PARAMETERS (homogeneity) ===============\n",
    "m, n = 3, 3  # 3 goods, 3 agents\n",
    "alpha_j = np.ones(m)  # preference weights\n",
    "omega_j = 0.5  # curvature param\n",
    "endowments = np.array([30.0, 30.0, 30.0])  # total endowments\n",
    "incomes = np.ones(n) * np.sum(endowments) / n  # equally distributed income\n",
    "\n",
    "eq_prices_homog = find_eq_prices(endowments, alpha_j, omega_j, incomes)\n",
    "print(\"Equilibrium Prices for Homogeneous Agents:\", eq_prices_homog)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By the FWT, every competitive equilibrium is Pareto efficient. Equilibrium prices align with marginal rates of substitution, and budget constraints enforce resource constraints. Therefore, the Arrow-Debreu equilibrium allocations coincide with the Pareto-efficient allocations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heterogeneous Agents, $m=n=3$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FOCs**\n",
    "> To find the demand for each good, we solve the Lagrangean:\n",
    "> $$\\mathcal{L}=\\sum_{j=1}^{m}\\alpha_j^i \\cdot \\frac{x_{ij}^{1+\\omega_j^i}}{1+\\omega_j^i}-\\lambda_i \\Big(\\sum_{j=1}^{m}p_j\\cdot x_{ij} - Y_i\\Big)$$\n",
    "> where $Y_i$ is the income of agent $i$. \n",
    "> The FOCs for this problem yield\n",
    "> $$ x_{ij} = \\frac{\\alpha_j^i \\cdot p_{j}^{-\\omega_j^i}}{\\sum_{k=1}^{m} \\alpha_k^i \\cdot p_{k}^{-\\omega_j^i} }\\cdot Y_i $$\n",
    "\n",
    "**Excess Demand**\n",
    "> Aggregate the demand across agents and compare it to the total endowment to find the excess demand for each good\n",
    "> $$z_j(p) =  \\sum_{i=1}^{n} x_{ij}(p) - e_j$$\n",
    "> Markets clear when $z_j(p)=0$ for all goods $j$.\n",
    "\n",
    "**Equilibrium Prices**\n",
    "> The goal is to find prices $p$ such that the excess demand is zero for all goods. This involves numerically solving a system of nonlinear equations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Individual agent's demand\n",
    "def agent_demand_heterog(p, alpha, omega, income):\n",
    "    demand = (alpha * (p ** -omega)) / (np.sum(alpha * (p ** -omega))) * income\n",
    "    return demand\n",
    "\n",
    "# Step 2: excess demand function\n",
    "def excess_demand_heterog(p, endowments, alpha, omega, incomes):\n",
    "    n, m = incomes.shape[0], endowments.shape[0]\n",
    "    total_demand = np.zeros(m)\n",
    "    \n",
    "    # Aggregate demand across all agents\n",
    "    for i in range(n):\n",
    "        demand_i = agent_demand_heterog(p, alpha[i], omega[i], incomes[i])\n",
    "        total_demand += demand_i\n",
    "    \n",
    "    excess_demand = total_demand - endowments\n",
    "    return excess_demand\n",
    "\n",
    "\n",
    "# Step 3: Solve for equilibrium prices where excess demand = 0\n",
    "def find_eq_prices_heterog(endowments, alpha, omega, incomes):\n",
    "    # Initial guess for prices\n",
    "    p0 = np.ones(endowments.shape[0])\n",
    "    \n",
    "    eq_prices = fsolve(lambda p: excess_demand_heterog(p, endowments, alpha, omega, incomes), p0)\n",
    "    \n",
    "    return eq_prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Equilibrium Prices for Heterogeneous Agents: [1.2410273  0.75897273 1.47233807]\n"
     ]
    }
   ],
   "source": [
    "# =============== DEFINE PARAMETERS (homogeneity) ===============\n",
    "m, n = 3, 3  # 3 goods, 3 agents\n",
    "alpha_j = np.array([[1.0, 0.8, 1.2],  # agent 1\n",
    "                    [1.1, 0.9, 1.3],  # agent 2\n",
    "                    [0.9, 0.7, 1.1]])  # agent 3\n",
    "\n",
    "omega_j = np.array([[0.3, 0.5, 0.7], \n",
    "                    [0.4, 0.6, 0.8],  \n",
    "                    [0.5, 0.4, 0.6]]) \n",
    "\n",
    "\n",
    "endowments = np.array([30.0, 30.0, 30.0])  # total endowments\n",
    "incomes = np.ones(n) * np.sum(endowments) / n  # equally distributed income\n",
    "\n",
    "eq_prices_heterog = find_eq_prices_heterog(endowments, alpha_j, omega_j, incomes)\n",
    "\n",
    "print(\"Equilibrium Prices for Heterogeneous Agents:\", eq_prices_heterog)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
