{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Econ 8210 Quant Macro, Homework 1\n",
    "## Part 2 - Solution Methods\n",
    "Haosi Shen, Fall 2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Housekeeping\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import torch\n",
    "\n",
    "np.random.seed(42) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing Pareto Efficient Allocations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider an endowment economy with $m$ different goods and $n$ agents. Each agent $i = 1, ..., n$ has an endowment $e_j^i >0$ for every $j = 1, ..., m$ and a utility function of the form\n",
    "$$ u^i (x) = \\sum_{j=1}^{m} \\alpha_j \\frac{x_{j}^{1+\\omega_j^i}}{1+\\omega_j^i} $$\n",
    "where $\\alpha_j > 0 > \\omega_j^i$ are agent-specific parameters.\n",
    "\n",
    "Given some social weights $\\lambda_i > 0$, solve for the social plannerâ€™s problem for $m = n = 3$ using the **Adam (Adaptive Moment Estimation)** method. Try different values of $\\alpha_j,\\;\\omega_j^i,\\;\\lambda_i$. \n",
    "\n",
    "$$ \\max_{\\{x^i\\}} \\; \\sum_{i=1}^{n} \\lambda_i u^{i}(x)$$\n",
    "\n",
    "Compute first the case where all the agents have the same parameters and\n",
    "social weights and later a case where there is a fair degree of heterogeneity.\n",
    "\n",
    "How does the method perform? How does heterogeneity in the agent-specific parameters\n",
    "affect the results?\n",
    "\n",
    "Can you handle the case where $m = n = 10$?\n",
    "\n",
    "> I choose to use **Adam** since it is more efficient for high-dimensional optimization problems and offers more stability and robustness. However, if we are only solving for the case of $m=n=3$, then the Newton-Raphson method might be more ideal since this problem is relatively smooth. Adam only requires gradient information and does not involve inverting the Hessian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility function for agent i\n",
    "def utility(x, alpha, omega):\n",
    "    return torch.sum(alpha * (x ** (1 + omega)) / (1 + omega))\n",
    "\n",
    "\n",
    "# Incorporate resource constraints for each good j\n",
    "# Define total endowments\n",
    "endowments = torch.tensor([30.0, 30.0, 30.0], dtype = torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case I: Homogeneous Agents\n",
    "\n",
    "> $m = n = 3$\n",
    "\n",
    "All agents $j$ have the same parameters $\\alpha_j, \\omega_j^i$ and social weights $\\lambda_j$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Social planner's objective, with Homogeneous Agents\n",
    "def social_planner_objective_homog(x, alpha_j, omega_j, lambda_i, endowments, penalty_weight = 1000):\n",
    "    total_utility = 0\n",
    "    total_allocations = torch.zeros(m)\n",
    "\n",
    "    for i in range(n):\n",
    "        x_i_softplus = torch.nn.functional.softplus(x[i]) # ensure nonneg allocations\n",
    "        total_utility += lambda_i[i] * utility(x_i_softplus, alpha_j, omega_j)\n",
    "        total_allocations += x_i_softplus\n",
    "\n",
    "    # penalize if RC is violated, i.e. total allocations > endowments\n",
    "    penalty = torch.sum(torch.clamp(total_allocations - endowments, min = 0) ** 2)\n",
    "\n",
    "    return -total_utility + penalty_weight * penalty   # maximization, so take negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 99: Objective = 41.48536682128906\n",
      "Iteration 199: Objective = 89.36106872558594\n",
      "Iteration 299: Objective = 157.54916381835938\n",
      "Iteration 399: Objective = 239.6324462890625\n",
      "Iteration 499: Objective = 331.9831237792969\n",
      "Iteration 599: Objective = 432.8163146972656\n",
      "Iteration 699: Objective = 541.0831909179688\n",
      "Iteration 799: Objective = 569.2794799804688\n",
      "Iteration 899: Objective = 569.2852172851562\n",
      "Iteration 999: Objective = 569.2862548828125\n",
      "\n",
      " Optimal allocations for Homogeneous Agents:\n",
      "            Good 1     Good 2     Good 3\n",
      "Agent 1   9.940836  10.041025  10.073340\n",
      "Agent 2  10.049340  10.083452   9.971957\n",
      "Agent 3  10.014572   9.880261   9.959444\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# =============== DEFINE PARAMETERS ===============\n",
    "m, n = 3, 3  # 3 goods, 3 agents\n",
    "alpha_j = torch.tensor([1.0, 1.0, 1.0], dtype = torch.float32) \n",
    "omega_j = torch.tensor([[0.5, 0.5, 0.5]] * n, dtype = torch.float32)\n",
    "lambda_i = torch.tensor([1.0, 1.0, 1.0], dtype = torch.float32)  # Pareto weights\n",
    "\n",
    "# initial allocations (using small positive values)\n",
    "x_i = torch.rand((n, m), requires_grad = True)\n",
    "\n",
    "# Set up optimizer\n",
    "optimizer = optim.Adam([x_i], lr = 0.01)\n",
    "\n",
    "# Optimization\n",
    "num_iterations = 1000\n",
    "for iteration in range(num_iterations):\n",
    "    optimizer.zero_grad()\n",
    "    objective = social_planner_objective_homog(x_i, alpha_j, omega_j, lambda_i, endowments)\n",
    "    objective.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (iteration+1) % 100 == 0:\n",
    "        print(f\"Iteration {iteration}: Objective = {-objective.item()}\")\n",
    "\n",
    "\n",
    "# Final optimal allocations\n",
    "final_allocations = torch.nn.functional.softplus(x_i).detach().numpy()\n",
    "\n",
    "df_final_homog = pd.DataFrame(\n",
    "    final_allocations,\n",
    "    index = [f'Agent {i+1}' for i in range(final_allocations.shape[0])],\n",
    "    columns = [f'Good {j+1}' for j in range(final_allocations.shape[1])]\n",
    ")\n",
    "\n",
    "print(\"\\n Optimal allocations for Homogeneous Agents:\")\n",
    "print(df_final_homog)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case II: Heterogeneous Agents \n",
    "> $m = n = 3$\n",
    "\n",
    "* Each agent $i$ has their own set of $\\alpha_j, \\omega_j^i$ parameters.\n",
    "* Pareto weights $\\lambda_i$ differ among agents.\n",
    "* Resource constraint remains the same. Total endowment for each good is 30."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Social planner's objective, with Heterogeneous Agents\n",
    "\n",
    "def social_planner_objective_heterog(x, alpha_j, omega_j, lambda_i, endowments, penalty_weight = 1000):\n",
    "    total_utility = 0\n",
    "    total_allocations = torch.zeros(m)\n",
    "\n",
    "    for i in range(n):\n",
    "        x_i_softplus = torch.nn.functional.softplus(x[i]) # ensure nonneg allocations\n",
    "        total_utility += lambda_i[i] * utility(x_i_softplus, alpha_j[i], omega_j[i])\n",
    "        total_allocations += x_i_softplus\n",
    "\n",
    "    # penalize if RC is violated, i.e. total allocations > endowments\n",
    "    penalty = torch.sum(torch.clamp(total_allocations - endowments, min = 0) ** 2)\n",
    "\n",
    "    return -total_utility + penalty_weight * penalty   # maximization, so take negative\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 999: Objective = 14.429256439208984\n",
      "Iteration 1999: Objective = 31.05494499206543\n",
      "Iteration 2999: Objective = 53.16339111328125\n",
      "Iteration 3999: Objective = 78.79014587402344\n",
      "Iteration 4999: Objective = 107.16829681396484\n",
      "Iteration 5999: Objective = 138.00628662109375\n",
      "Iteration 6999: Objective = 171.1595458984375\n",
      "Iteration 7999: Objective = 206.53012084960938\n",
      "Iteration 8999: Objective = 219.7765655517578\n",
      "Iteration 9999: Objective = 225.86273193359375\n",
      "\n",
      " Optimal allocations for Heterogeneous Agents:\n",
      "            Good 1     Good 2     Good 3\n",
      "Agent 1   9.081430   9.787652  10.099073\n",
      "Agent 2  10.296638  11.073445  11.160144\n",
      "Agent 3  10.623183   9.140093   8.743629\n"
     ]
    }
   ],
   "source": [
    "# =============== DEFINE PARAMETERS ===============\n",
    "alpha_j = torch.tensor([[1.0, 0.8, 1.2],  # agent 1\n",
    "                        [1.1, 0.9, 1.3],  # agent 2\n",
    "                        [0.9, 0.7, 1.1]])  # agent 3\n",
    "\n",
    "omega_j = torch.tensor([[0.3, 0.5, 0.7], \n",
    "                        [0.4, 0.6, 0.8],  \n",
    "                        [0.5, 0.4, 0.6]]) \n",
    "\n",
    "lambda_i = torch.tensor([0.9, 1.1, 1.0])  # Social weights\n",
    "\n",
    "\n",
    "# initial allocations (using small positive values)\n",
    "x_i = torch.rand((n, m), requires_grad = True)\n",
    "#x_i = (torch.rand((n, m), requires_grad=True) * endowments / n).clone().detach().requires_grad_(True)\n",
    "\n",
    "# Set up optimizer\n",
    "optimizer = optim.Adam([x_i], lr = 0.001)\n",
    "\n",
    "# Optimization\n",
    "num_iterations = 10000\n",
    "for iteration in range(num_iterations):\n",
    "    optimizer.zero_grad()\n",
    "    objective = social_planner_objective_heterog(x_i, alpha_j, omega_j, lambda_i, endowments)\n",
    "    objective.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (iteration+1) % 1000 == 0:\n",
    "        print(f\"Iteration {iteration}: Objective = {-objective.item()}\")\n",
    "\n",
    "\n",
    "# Final optimal allocations\n",
    "final_allocations = torch.nn.functional.softplus(x_i).detach().numpy()\n",
    "\n",
    "df_final_heterog = pd.DataFrame(\n",
    "    final_allocations,\n",
    "    index = [f'Agent {i+1}' for i in range(final_allocations.shape[0])],\n",
    "    columns = [f'Good {j+1}' for j in range(final_allocations.shape[1])]\n",
    ")\n",
    "\n",
    "print(\"\\n Optimal allocations for Heterogeneous Agents:\")\n",
    "print(df_final_heterog)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The `Adam` optimizer performs fairly well for both the homogeneous and heterogeneous agents cases. The computations are fast and produce sufficiently accurate results. \n",
    "> \n",
    "> Introducing heterogeneity in the agent-specific parameters does affect the socially optimal allocations, and significantly slow down convergence of optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case III: 10 Agents, 10 Goods\n",
    "\n",
    "> $m=n=10$\n",
    "\n",
    "Since the `Adam` optimizer generally works well for higher-dimension problems, we now try computing the Pareto efficient allocations of a 10-agent 10-good economy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Consider homogeneity across agents.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 99: Objective = 157.95803833007812\n",
      "Iteration 199: Objective = 325.2500915527344\n",
      "Iteration 299: Objective = 345.5758361816406\n",
      "Iteration 399: Objective = 346.7998962402344\n",
      "Iteration 499: Objective = 346.927734375\n",
      "Iteration 599: Objective = 346.99945068359375\n",
      "Iteration 699: Objective = 346.41583251953125\n",
      "Iteration 799: Objective = 347.06060791015625\n",
      "Iteration 899: Objective = 346.8817138671875\n",
      "Iteration 999: Objective = 346.8741149902344\n",
      "\n",
      " Optimal allocations for Homogeneous Agents:\n",
      "            Good 1    Good 2    Good 3    Good 4    Good 5    Good 6  \\\n",
      "Agent 1   2.852486  3.065950  2.721496  3.140219  2.844044  3.327388   \n",
      "Agent 2   3.098233  2.968297  2.921544  2.665549  2.999330  2.875577   \n",
      "Agent 3   2.542581  3.226127  2.840879  3.262613  3.279150  2.779028   \n",
      "Agent 4   3.382443  2.729536  3.246885  2.885556  2.965294  3.184775   \n",
      "Agent 5   3.015020  3.292438  3.246809  3.255260  2.715430  2.876262   \n",
      "Agent 6   2.726313  2.929272  2.927523  2.957928  3.173136  2.718218   \n",
      "Agent 7   2.541137  2.837680  2.744239  3.041883  3.154484  2.965400   \n",
      "Agent 8   3.438016  2.968727  2.889905  3.099016  2.855495  2.743480   \n",
      "Agent 9   3.476681  3.321712  3.616022  3.005432  3.111307  3.153639   \n",
      "Agent 10  2.927876  2.641971  2.845573  2.629141  2.903224  3.301605   \n",
      "\n",
      "            Good 7    Good 8    Good 9   Good 10  \n",
      "Agent 1   2.863472  3.181206  2.713175  3.324302  \n",
      "Agent 2   2.912299  3.173266  2.734358  2.546613  \n",
      "Agent 3   2.917158  3.012063  3.293152  3.196243  \n",
      "Agent 4   2.592449  2.733169  3.203780  2.759444  \n",
      "Agent 5   3.192063  3.183831  3.427194  2.501928  \n",
      "Agent 6   3.475843  2.642646  2.748634  3.321702  \n",
      "Agent 7   2.838911  2.924949  2.934670  3.145350  \n",
      "Agent 8   3.057176  3.259622  2.939758  3.100627  \n",
      "Agent 9   2.959941  2.835932  2.835417  2.945288  \n",
      "Agent 10  3.191571  2.980096  3.169784  3.159371  \n"
     ]
    }
   ],
   "source": [
    "m, n = 10, 10  # 10 goods, 10 agents\n",
    "endowments = torch.tensor([30.0] * m)  # total endowments for 10 goods\n",
    "\n",
    "alpha_j = torch.tensor([1.0] * m)  \n",
    "omega_j = torch.tensor([0.5] * m)  \n",
    "lambda_i = torch.tensor([1.0] * n)\n",
    "\n",
    "x_i = torch.rand((n, m), requires_grad = True)  # initial allocations\n",
    "\n",
    "optimizer = optim.Adam([x_i], lr = 0.01)\n",
    "\n",
    "# Optimization\n",
    "num_iterations = 1000\n",
    "for iteration in range(num_iterations):\n",
    "    optimizer.zero_grad()\n",
    "    objective = social_planner_objective_homog(x_i, alpha_j, omega_j, lambda_i, endowments)\n",
    "    objective.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (iteration+1) % 100 == 0:\n",
    "        print(f\"Iteration {iteration}: Objective = {-objective.item()}\")\n",
    "\n",
    "\n",
    "# Final optimal allocations\n",
    "final_allocations = torch.nn.functional.softplus(x_i).detach().numpy()\n",
    "\n",
    "df_final_homog = pd.DataFrame(\n",
    "    final_allocations,\n",
    "    index = [f'Agent {i+1}' for i in range(final_allocations.shape[0])],\n",
    "    columns = [f'Good {j+1}' for j in range(final_allocations.shape[1])]\n",
    ")\n",
    "\n",
    "print(\"\\n Optimal allocations for Homogeneous Agents:\")\n",
    "print(df_final_homog)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now, try the heterogeneous agents case.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heterogeneous agent-specific parameters\n",
    "alpha_j = torch.tensor([\n",
    "    [1.0, 0.8, 1.2, 0.9, 1.1, 0.7, 1.3, 0.95, 1.05, 1.15] for _ in range(n)]) \n",
    "\n",
    "omega_j = torch.tensor([\n",
    "    [0.3, 0.4, 0.5, 0.35, 0.45, 0.55, 0.25, 0.6, 0.65, 0.5] for _ in range(n)])\n",
    "\n",
    "lambda_i = torch.tensor([0.9, 1.1, 0.8, 1.2, 1.0, 0.85, 1.15, 0.95, 1.05, 1.0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 999: Objective = 168.5098876953125\n",
      "Iteration 1999: Objective = 333.4227294921875\n",
      "Iteration 2999: Objective = 350.2723388671875\n",
      "Iteration 3999: Objective = 360.41552734375\n",
      "Iteration 4999: Objective = 384.34906005859375\n",
      "Iteration 5999: Objective = 425.4910888671875\n",
      "Iteration 6999: Objective = 477.1968994140625\n",
      "Iteration 7999: Objective = 530.7722778320312\n",
      "Iteration 8999: Objective = 582.2130126953125\n",
      "Iteration 9999: Objective = 628.9527587890625\n",
      "\n",
      " Optimal allocations for Heterogeneous Agents:\n",
      "            Good 1    Good 2    Good 3    Good 4     Good 5    Good 6  \\\n",
      "Agent 1   0.090776  0.170010  0.048248  0.128603   0.107813  0.091446   \n",
      "Agent 2   7.479658  6.854785  8.812072  8.476297   8.428061  8.622780   \n",
      "Agent 3   0.093248  0.119695  0.054456  0.099403   0.077333  0.084708   \n",
      "Agent 4   9.115497  9.180102  9.731377  9.565689   9.611230  9.415743   \n",
      "Agent 5   1.066900  0.748357  0.208702  0.244601   0.827350  1.838966   \n",
      "Agent 6   0.072845  0.089515  0.038561  0.145964   0.096440  0.111948   \n",
      "Agent 7   8.796108  9.401570  9.041632  8.866896  10.200636  9.176359   \n",
      "Agent 8   0.114619  0.413291  0.085137  0.398127   0.067714  0.133735   \n",
      "Agent 9   2.487896  1.584264  1.894304  1.573044   0.200711  0.367979   \n",
      "Agent 10  0.683325  1.439116  0.087440  0.502216   0.384165  0.157436   \n",
      "\n",
      "            Good 7     Good 8     Good 9   Good 10  \n",
      "Agent 1   0.204454   0.082741   0.049575  0.083430  \n",
      "Agent 2   6.320855   1.732735   8.685026  8.955709  \n",
      "Agent 3   0.103616   0.059855   0.039443  0.051586  \n",
      "Agent 4   8.616991  10.226381  10.205626  9.233255  \n",
      "Agent 5   0.327434   0.206710   1.433471  2.050171  \n",
      "Agent 6   0.143624   0.038553   0.043839  0.071200  \n",
      "Agent 7   8.576061  10.246546   9.187946  9.143661  \n",
      "Agent 8   0.159602   0.084092   0.048477  0.061019  \n",
      "Agent 9   5.313690   7.152207   0.235614  0.092986  \n",
      "Agent 10  0.234800   0.171840   0.073359  0.258787  \n"
     ]
    }
   ],
   "source": [
    "x_i = torch.rand((n, m), requires_grad=True)\n",
    "optimizer = optim.Adam([x_i], lr = 0.001)\n",
    "\n",
    "# Optimization\n",
    "num_iterations = 10000\n",
    "for iteration in range(num_iterations):\n",
    "    optimizer.zero_grad()\n",
    "    objective = social_planner_objective_heterog(x_i, alpha_j, omega_j, lambda_i, endowments)\n",
    "    objective.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (iteration+1) % 1000 == 0:\n",
    "        print(f\"Iteration {iteration}: Objective = {-objective.item()}\")\n",
    "\n",
    "\n",
    "# Final optimal allocations\n",
    "final_allocations = torch.nn.functional.softplus(x_i).detach().numpy()\n",
    "\n",
    "df_final_heterog = pd.DataFrame(\n",
    "    final_allocations,\n",
    "    index = [f'Agent {i+1}' for i in range(final_allocations.shape[0])],\n",
    "    columns = [f'Good {j+1}' for j in range(final_allocations.shape[1])]\n",
    ")\n",
    "\n",
    "print(\"\\n Optimal allocations for Heterogeneous Agents:\")\n",
    "print(df_final_heterog)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The method works well for $m=n=10$, even after we introduce household heterogeneity. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing Equilibrium Allocations\n",
    "\n",
    "Using the same model as in the previous exercise. Find the equilibrium prices for each good $p^j$.\n",
    "\n",
    "1. Solve for the first-order conditions of each agent.\n",
    "1. Aggregate the excess demands. \n",
    "1. Solve the resulting system of nonlinear equations, when all markets clear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Homogeneous Agents, $m=n=3$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FOCs**\n",
    "> To find the demand for each good, we solve the Lagrangean:\n",
    "> $$\\mathcal{L}=\\sum_{j=1}^{m}\\alpha_j \\cdot \\frac{x_{ij}^{1+\\omega}}{1+\\omega}-\\lambda_i \\Big(\\sum_{j=1}^{m}p_j\\cdot x_{ij} - Y_i\\Big)$$\n",
    "> where $Y_i$ is the income of agent $i$. \n",
    "> The FOCs for this problem yield\n",
    "> $$ x_{ij} = \\frac{\\alpha_j \\cdot p_{j}^{-\\omega}}{\\sum_{k=1}^{m} \\alpha_k \\cdot p_{k}^{-\\omega} }\\cdot Y_i $$\n",
    "\n",
    "**Excess Demand**\n",
    "> Aggregate the demand across agents and compare it to the total endowment to find the excess demand for each good\n",
    "> $$z_j(p) =  \\sum_{i=1}^{n} x_{ij}(p) - e_j$$\n",
    "> Markets clear when $z_j(p)=0$ for all goods $j$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heterogeneous Agents, $m=n=3$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
