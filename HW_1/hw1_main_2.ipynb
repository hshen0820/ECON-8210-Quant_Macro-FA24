{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Econ 8210 Quant Macro, Homework 1\n",
    "## Part 2 - Solution Methods\n",
    "Haosi Shen, Fall 2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Housekeeping\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import torch\n",
    "\n",
    "np.random.seed(42) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing Pareto Efficient Allocations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider an endowment economy with $m$ different goods and $n$ agents. Each agent $i = 1, ..., n$ has an endowment $e_j^i >0$ for every $j = 1, ..., m$ and a utility function of the form\n",
    "$$ u^i (x) = \\sum_{j=1}^{m} \\alpha_j \\frac{x_{j}^{1+\\omega_j^i}}{1+\\omega_j^i} $$\n",
    "where $\\alpha_j > 0 > \\omega_j^i$ are agent-specific parameters.\n",
    "\n",
    "Given some social weights $\\lambda_i > 0$, solve for the social plannerâ€™s problem for $m = n = 3$ using the **Adam (Adaptive Moment Estimation)** method. Try different values of $\\alpha_j,\\;\\omega_j^i,\\;\\lambda_i$. \n",
    "\n",
    "$$ \\max_{\\{x^i\\}} \\; \\sum_{i=1}^{n} \\lambda_i u^{i}(x)$$\n",
    "\n",
    "Compute first the case where all the agents have the same parameters and\n",
    "social weights and later a case where there is a fair degree of heterogeneity.\n",
    "\n",
    "How does the method perform? How does heterogeneity in the agent-specific parameters\n",
    "affect the results?\n",
    "\n",
    "Can you handle the case where $m = n = 10$?\n",
    "\n",
    "> I choose to use **Adam** since it is more efficient for high-dimensional optimization problems and offers more stability and robustness. However, if we are only solving for the case of $m=n=3$, then the Newton-Raphson method might be more ideal since this problem is relatively smooth. Adam only requires gradient information and does not involve inverting the Hessian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility function for agent i\n",
    "def utility(x, alpha, omega):\n",
    "    return torch.sum(alpha * (x ** (1 + omega)) / (1 + omega))\n",
    "\n",
    "\n",
    "# Incorporate resource constraints for each good j\n",
    "# Define total endowments\n",
    "endowments = torch.tensor([30.0, 30.0, 30.0], dtype = torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Social planner's objective, with RC enforced\n",
    "def social_planner_objective(x, alpha_j, omega_j, lambda_i, endowments, penalty_weight = 1000):\n",
    "    total_utility = 0\n",
    "    total_allocations = torch.zeros(m)\n",
    "\n",
    "    for i in range(n):\n",
    "        x_i_softplus = torch.nn.functional.softplus(x[i]) # ensure nonneg allocations\n",
    "        total_utility += lambda_i[i] * utility(x_i_softplus, alpha_j, omega_j)\n",
    "        total_allocations += x_i_softplus\n",
    "\n",
    "    # penalize if RC is violated, i.e. total allocations > endowments\n",
    "    penalty = torch.sum(torch.clamp(total_allocations - endowments, min = 0) ** 2)\n",
    "\n",
    "    return -total_utility + penalty_weight * penalty   # maximization, so take negative\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case I: Homogeneous Agents\n",
    "\n",
    "> $m = n = 3$\n",
    "\n",
    "All agents $j$ have the same parameters $\\alpha_j, \\omega_j^i$ and social weights $\\lambda_j$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: Objective = 16.002307891845703\n",
      "Iteration 100: Objective = 40.352108001708984\n",
      "Iteration 200: Objective = 88.16224670410156\n",
      "Iteration 300: Objective = 156.52291870117188\n",
      "Iteration 400: Objective = 238.8422393798828\n",
      "Iteration 500: Objective = 331.4263000488281\n",
      "Iteration 600: Objective = 432.4744873046875\n",
      "Iteration 700: Objective = 540.9359130859375\n",
      "Iteration 800: Objective = 569.283935546875\n",
      "Iteration 900: Objective = 569.2868041992188\n",
      "Iteration 1000: Objective = 569.2880249023438\n",
      "Iteration 1100: Objective = 569.28955078125\n",
      "Iteration 1200: Objective = 569.2914428710938\n",
      "Iteration 1300: Objective = 569.2937622070312\n",
      "Iteration 1400: Objective = 569.2967529296875\n",
      "Optimal allocations:\n",
      "[[10.137773   9.8984585 10.027572 ]\n",
      " [ 9.845664  10.18302    9.949477 ]\n",
      " [10.021302   9.9232645 10.027695 ]]\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# =============== DEFINE PARAMETERS ===============\n",
    "m, n = 3, 3  # 3 goods, 3 agents\n",
    "alpha_j = torch.tensor([1.0, 1.0, 1.0], dtype = torch.float32) \n",
    "omega_j = torch.tensor([[0.5, 0.5, 0.5]] * n, dtype = torch.float32)\n",
    "lambda_i = torch.tensor([1.0, 1.0, 1.0], dtype = torch.float32)  # Pareto weights\n",
    "\n",
    "# initial allocations (using small positive values)\n",
    "x_i = torch.rand((n, m), requires_grad = True)\n",
    "\n",
    "# Set up optimizer\n",
    "optimizer = optim.Adam([x_i], lr = 0.01)\n",
    "\n",
    "# Optimization\n",
    "num_iterations = 1500\n",
    "for iteration in range(num_iterations):\n",
    "    optimizer.zero_grad()\n",
    "    objective = social_planner_objective(x_i, alpha_j, omega_j, lambda_i, endowments)\n",
    "    objective.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if iteration % 100 == 0:\n",
    "        print(f\"Iteration {iteration}: Objective = {-objective.item()}\")\n",
    "\n",
    "\n",
    "# Final optimal allocations\n",
    "final_allocations = torch.nn.functional.softplus(x_i).detach().numpy()\n",
    "\n",
    "print(\"Optimal allocations:\")\n",
    "print(final_allocations)\n",
    "# print(\"Total allocations per good:\")\n",
    "# print(final_allocations.sum(axis=0))\n",
    "# print(\"Endowments:\")\n",
    "# print(endowments.numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(final_allocations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case II: Heterogeneous Agents \n",
    "> $m = n = 3$\n",
    "\n",
    "Parameters $\\alpha_j, \\omega_j^i$\n",
    "Social weights $\\lambda_j$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============== DEFINE PARAMETERS ===============\n",
    "alpha_j_het = torch.tensor([1.0, 0.8, 1.2], dtype = torch.float32) \n",
    "omega_j_het = torch.tensor([[0.3, 0.5, 0.7]] * n, dtype = torch.float32)\n",
    "lambda_i_het = torch.tensor([0.9, 1.0, 1.01], dtype = torch.float32)  # Pareto weights\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
